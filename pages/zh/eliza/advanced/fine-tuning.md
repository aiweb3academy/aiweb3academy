# ğŸ¯ å¾®è°ƒæŒ‡å—

## æ¦‚è¿°
Eliza æ”¯æŒå¤šä¸ª AI æ¨¡å‹æä¾›å•†ï¼Œå¹¶ä¸ºå¾®è°ƒæ¨¡å‹è¡Œä¸ºã€åµŒå…¥ç”Ÿæˆå’Œæ€§èƒ½ä¼˜åŒ–æä¾›äº†å¹¿æ³›çš„é…ç½®é€‰é¡¹ã€‚

## æ¨¡å‹æä¾›å•†

Eliza é€šè¿‡ä¸€ä¸ªçµæ´»çš„é…ç½®ç³»ç»Ÿæ”¯æŒå¤šä¸ªæ¨¡å‹æä¾›å•†ï¼š
```typescript
enum ModelProviderName {
  OPENAI,
  ANTHROPIC,
  CLAUDE_VERTEX,
  GROK,
  GROQ,
  LLAMACLOUD,
  LLAMALOCAL,
  GOOGLE,
  REDPILL,
  OPENROUTER,
  HEURIST,
}
```

### æä¾›å•†é…ç½®
æ¯ä¸ªæä¾›å•†éƒ½æœ‰ç‰¹å®šçš„è®¾ç½®ï¼š
```typescript
const models = {
  [ModelProviderName.ANTHROPIC]: {
    settings: {
      stop: [],
      maxInputTokens: 200000,
      maxOutputTokens: 8192,
      frequency_penalty: 0.0,
      presence_penalty: 0.0,
      temperature: 0.3,
    },
    endpoint: "https://api.anthropic.com/v1",
    model: {
      [ModelClass.SMALL]: "claude-3-5-haiku",
      [ModelClass.MEDIUM]: "claude-3-5-sonnet-20241022",
      [ModelClass.LARGE]: "claude-3-5-opus-20240229",
    },
  },
  //... å…¶ä»–æä¾›å•†
};
```

## æ¨¡å‹ç±»åˆ«
æ ¹æ®å…¶èƒ½åŠ›å°†æ¨¡å‹åˆ†ç±»ä¸ºä¸åŒçš„ç±»åˆ«ï¼š
```typescript
enum ModelClass {
    SMALL,    // å¿«é€Ÿï¼Œé€‚ç”¨äºç®€å•ä»»åŠ¡
    MEDIUM,   // æ€§èƒ½å’Œèƒ½åŠ›å¹³è¡¡
    LARGE,    // èƒ½åŠ›æœ€å¼ºä½†æ›´æ…¢/æ›´æ˜‚è´µ
    EMBEDDING // ä¸“é—¨ç”¨äºå‘é‡åµŒå…¥
    IMAGE     // å›¾åƒç”Ÿæˆèƒ½åŠ›
}
```

## åµŒå…¥ç³»ç»Ÿ

### é…ç½®
```typescript
const embeddingConfig = {
  dimensions: 1536,
  modelName: "text-embedding-3-small",
  cacheEnabled: true,
};
```

### å®ç°
```typescript
async function embed(runtime: IAgentRuntime, input: string): Promise<number[]> {
  // å…ˆæ£€æŸ¥ç¼“å­˜
  const cachedEmbedding = await retrieveCachedEmbedding(runtime, input);
  if (cachedEmbedding) return cachedEmbedding;

  // ç”Ÿæˆæ–°çš„åµŒå…¥
  const response = await runtime.fetch(
    `${runtime.modelProvider.endpoint}/embeddings`,
    {
      method: "POST",
      headers: {
        Authorization: `Bearer ${runtime.token}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        input,
        model: runtime.modelProvider.model.EMBEDDING,
        dimensions: 1536,
      }),
    },
  );

  const data = await response.json();
  return data?.data?.[0].embedding;
}
```

## å¾®è°ƒé€‰é¡¹

### æ¸©åº¦æ§åˆ¶
é…ç½®æ¨¡å‹çš„åˆ›é€ æ€§ä¸ç¡®å®šæ€§ï¼š
```typescript
const temperatureSettings = {
  creative: {
    temperature: 0.8,
    frequency_penalty: 0.7,
    presence_penalty: 0.7,
  },
  balanced: {
    temperature: 0.5,
    frequency_penalty: 0.3,
    presence_penalty: 0.3,
  },
  precise: {
    temperature: 0.2,
    frequency_penalty: 0.0,
    presence_penalty: 0.0,
  },
};
```

### ä¸Šä¸‹æ–‡çª—å£
ç®¡ç†ä»¤ç‰Œé™åˆ¶ï¼š
```typescript
const contextSettings = {
  OPENAI: {
    maxInputTokens: 128000,
    maxOutputTokens: 8192,
  },
  ANTHROPIC: {
    maxInputTokens: 200000,
    maxOutputTokens: 8192,
  },
  LLAMALOCAL: {
    maxInputTokens: 32768,
    maxOutputTokens: 8192,
  },
};
```

## æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜ç­–ç•¥
```typescript
class EmbeddingCache {
  private cache: NodeCache;
  private cacheDir: string;

  constructor() {
    this.cache = new NodeCache({ stdTTL: 300 }); // 5 åˆ†é’Ÿçš„ TTL
    this.cacheDir = path.join(__dirname, "cache");
  }

  async get(key: string): Promise<number[] | null> {
    // å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
    const cached = this.cache.get<number[]>(key);
    if (cached) return cached;

    // æ£€æŸ¥ç£ç›˜ç¼“å­˜
    return this.readFromDisk(key);
  }

  async set(key: string, embedding: number[]): Promise<void> {
    this.cache.set(key, embedding);
    await this.writeToDisk(key, embedding);
  }
}
```

### æ¨¡å‹é€‰æ‹©
```typescript
async function selectOptimalModel(
  task: string,
  requirements: ModelRequirements,
): Promise<ModelClass> {
  if (requirements.speed === "fast") {
    return ModelClass.SMALL;
  } else if (requirements.complexity === "high") {
    return ModelClass.LARGE;
  }
  return ModelClass.MEDIUM;
}
```

## æä¾›å•†ç‰¹å®šçš„ä¼˜åŒ–

### OpenAI
```typescript
const openAISettings = {
  endpoint: "https://api.openai.com/v1",
  settings: {
    stop: [],
    maxInputTokens: 128000,
    maxOutputTokens: 8192,
    frequency_penalty: 0.0,
    presence_penalty: 0.0,
    temperature: 0.6,
  },
  model: {
    [ModelClass.SMALL]: "gpt-4o-mini",
    [ModelClass.MEDIUM]: "gpt-4o",
    [ModelClass.LARGE]: "gpt-4o",
    [ModelClass.EMBEDDING]: "text-embedding-3-small",
    [ModelClass.IMAGE]: "dall-e-3",
  },
};
```

### Anthropic
```typescript
const anthropicSettings = {
  endpoint: "https://api.anthropic.com/v1",
  settings: {
    stop: [],
    maxInputTokens: 200000,
    maxOutputTokens: 8192,
    temperature: 0.3,
  },
  model: {
    [ModelClass.SMALL]: "claude-3-5-haiku",
    [ModelClass.MEDIUM]: "claude-3-5-sonnet-20241022",
    [ModelClass.LARGE]: "claude-3-5-opus-20240229",
  },
};
```

### æœ¬åœ° LLM
```typescript
const llamaLocalSettings = {
  settings: {
    stop: ["<|eot_id|>", "<|eom_id|>"],
    maxInputTokens: 32768,
    maxOutputTokens: 8192,
    repetition_penalty: 0.0,
    temperature: 0.3,
  },
  model: {
    [ModelClass.SMALL]: "NousResearch/Hermes-3-Llama-3.1-8B-GGUF",
    [ModelClass.MEDIUM]: "NousResearch/Hermes-3-Llama-3.1-8B-GGUF",
    [ModelClass.LARGE]: "NousResearch/Hermes-3-Llama-3.1-8B-GGUF",
    [ModelClass.EMBEDDING]: "togethercomputer/m2-bert-80M-32k-retrieval",
  },
};
```

### Heurist æä¾›å•†
```typescript
const heuristSettings = {
  settings: {
    stop: [],
    maxInputTokens: 32768,
    maxOutputTokens: 8192,
    repetition_penalty: 0.0,
    temperature: 0.7,
  },
  imageSettings: {
    steps: 20,
  },
  endpoint: "https://llm-gateway.heurist.xyz",
  model: {
    [ModelClass.SMALL]: "hermes-3-llama3.1-8b",
    [ModelClass.MEDIUM]: "mistralai/mixtral-8x7b-instruct",
    [ModelClass.LARGE]: "nvidia/llama-3.1-nemotron-70b-instruct",
    [ModelClass.EMBEDDING]: "", // ç¨åæ·»åŠ 
    [ModelClass.IMAGE]: "FLUX.1-dev",
  },
};
```

## æµ‹è¯•å’ŒéªŒè¯

### åµŒå…¥æµ‹è¯•
```typescript
async function validateEmbedding(
  embedding: number[],
  expectedDimensions: number = 1536,
): Promise<boolean> {
  if (!Array.isArray(embedding)) return false;
  if (embedding.length!== expectedDimensions) return false;
  if (embedding.some((n) => typeof n!== "number")) return false;
  return true;
}
```

### æ¨¡å‹æ€§èƒ½æµ‹è¯•
```typescript
async function benchmarkModel(
  runtime: IAgentRuntime,
  modelClass: ModelClass,
  testCases: TestCase[]): Promise<BenchmarkResults> {
  const results = {
    latency: [],
    tokenUsage: [],
    accuracy: [],
  };

  for (const test of testCases) {
    const start = Date.now();
    const response = await runtime.generateText({
      context: test.input,
      modelClass,
    });
    results.latency.push(Date.now() - start);
    //... å…¶ä»–æŒ‡æ ‡
  }

  return results;
}
```

## æœ€ä½³å®è·µ

### æ¨¡å‹é€‰æ‹©æŒ‡å—

1. **ä»»åŠ¡å¤æ‚æ€§**
    - å¯¹äºç®€å•ã€å¿«é€Ÿçš„å“åº”ä½¿ç”¨ SMALL æ¨¡å‹ã€‚
    - å¯¹äºæ€§èƒ½å¹³è¡¡ä½¿ç”¨ MEDIUM æ¨¡å‹ã€‚
    - å¯¹äºå¤æ‚æ¨ç†ä½¿ç”¨ LARGE æ¨¡å‹ã€‚

2. **ä¸Šä¸‹æ–‡ç®¡ç†**
    - ä¿æŒæç¤ºç®€æ´å’Œé›†ä¸­ã€‚
    - æœ‰æ•ˆä½¿ç”¨ä¸Šä¸‹æ–‡çª—å£ã€‚
    - å®ç°é€‚å½“çš„ä¸Šä¸‹æ–‡æˆªæ–­ã€‚

3. **æ¸©åº¦è°ƒæ•´**
    - å¯¹äºäº‹å®æ€§å“åº”é™ä½æ¸©åº¦ã€‚
    - å¯¹äºåˆ›é€ æ€§ä»»åŠ¡æé«˜æ¸©åº¦ã€‚
    - æ ¹æ®ç”¨ä¾‹è¿›è¡Œå¹³è¡¡ã€‚

### æ€§èƒ½ä¼˜åŒ–

1. **ç¼“å­˜ç­–ç•¥**
    - ä¸ºé¢‘ç¹è®¿é—®çš„å†…å®¹ç¼“å­˜åµŒå…¥ã€‚
    - å®ç°åˆ†å±‚ç¼“å­˜ï¼ˆå†…å­˜/ç£ç›˜ï¼‰ã€‚
    - å®šæœŸæ¸…ç†ç¼“å­˜ã€‚

2. **èµ„æºç®¡ç†**
    - ç›‘æ§ä»¤ç‰Œä½¿ç”¨æƒ…å†µã€‚
    - å®æ–½é€Ÿç‡é™åˆ¶ã€‚
    - ä¼˜åŒ–æ‰¹å¤„ç†ã€‚

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ä»¤ç‰Œé™åˆ¶**
```typescript
function handleTokenLimit(error: Error) {
  if (error.message.includes("token limit")) {
    return truncateAndRetry();
  }
}
```

2. **åµŒå…¥é”™è¯¯**
```typescript
function handleEmbeddingError(error: Error) {
  if (error.message.includes("dimension mismatch")) {
    return regenerateEmbedding();
  }
}
```

3. **æ¨¡å‹å¯ç”¨æ€§**
```typescript
async function handleModelFailover(error: Error) {
  if (error.message.includes("model not available")) {
    return switchToFallbackModel();
  }
}
```
